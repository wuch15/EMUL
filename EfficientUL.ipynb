{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "from sklearn.metrics import *\n",
    "import keras\n",
    "from keras.layers import *\n",
    "from keras.models import Model\n",
    "from keras.optimizers import *\n",
    "from keras import backend as K\n",
    "from keras.utils.np_utils import to_categorical\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SHARD_NUM=10\n",
    "SAMPLE_DATA_RATIO=0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('adult.data','r')as f:\n",
    "    traindata=[[p.strip() for  p in x.strip('\\n').strip('.').split(',')] for x in f.readlines()]\n",
    "\n",
    "with open('adult.test','r')as f:\n",
    "    testdata=[[p.strip() for  p in x.strip('\\n').strip('.').split(',')] for x in f.readlines()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datadict={i:{} for i in range(15)}\n",
    "\n",
    "for data in traindata+testdata:\n",
    "    for i in range(15):\n",
    "        if data[i] not in datadict[i]:\n",
    "            datadict[i][data[i]]=len(datadict[i])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cateid=[1,3,5,6,7,8,9,13]\n",
    "alldata=[]\n",
    "alllabel=[]\n",
    "for data in traindata+testdata:\n",
    "    tp=[]\n",
    "    for i in range(14):\n",
    "        if i in cateid:\n",
    "            tp+=to_categorical(datadict[i][data[i]],len(datadict[i])).tolist()\n",
    "        else:\n",
    "            tp.append(float(data[i]))\n",
    "    \n",
    "    alldata.append(tp)\n",
    "    alllabel.append(datadict[14][data[14]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alldata=np.array(alldata)\n",
    "alllabel=np.array(alllabel)\n",
    "for i in range(len(alldata[0])):\n",
    "    alldata[:,i]/=np.max(alldata[:,i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class DNN(Layer):\n",
    "    def __init__(self, dim=200,**kwargs):\n",
    "        #self.init = initializers.get('normal')\n",
    "        self.dim = dim\n",
    "        super(DNN, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        dim = self.dim\n",
    "        self.W1 = tf.Variable(new_para_list[0],trainable=True)\n",
    "        self.b1 = tf.Variable(new_para_list[1],trainable=True)\n",
    "        self.W2 = tf.Variable(new_para_list[2],trainable=False)\n",
    "        self.b2 = tf.Variable(new_para_list[3],trainable=False)        \n",
    "        self.wa1 = self.add_weight(name='wa1',  shape=(new_para_list[0].shape[0],1,1),initializer='zeros', trainable=True)\n",
    "        self.ba1 = self.add_weight(name='ba1',   shape=(new_para_list[0].shape[0],1), initializer='zeros',trainable=True)\n",
    "        self.wa2 = self.add_weight(name='wa2',   shape=(new_para_list[0].shape[0],1,1),initializer='zeros',trainable=True)     \n",
    "        self.ba2 = self.add_weight(name='ba2',   shape=(new_para_list[0].shape[0],1),initializer='zeros',trainable=True)     \n",
    " \n",
    "        self.trainable_weights = [self.wa1,self.ba1,self.wa2,self.ba2]\n",
    "        super(DNN,self).build(input_shape)  # be sure you call this somewhere!\n",
    "\n",
    "    def call(self, key, mask=None, **kwargs):\n",
    "        \n",
    "        W1=K.sum(K.softmax(self.wa1,axis=0)*self.W1,axis=0)\n",
    "        b1=K.sum(K.softmax(self.ba1,axis=0)*self.b1,axis=0)\n",
    "        W2=K.sum(K.softmax(self.wa2,axis=0)*self.W2,axis=0)\n",
    "        b2=K.sum(K.softmax(self.ba2,axis=0)*self.b2,axis=0)\n",
    "        logit = K.dot(K.relu(K.dot(key, W1) + b1), W2)+b2\n",
    "        logit = K.sigmoid(logit)\n",
    "        \n",
    "        return logit\n",
    "\n",
    "    def compute_mask(self, input, input_mask=None):\n",
    "        return None\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return input_shape[0], self.b2.shape[-1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_acc=[]\n",
    "all_pred=[]\n",
    "all_para=[]\n",
    "\n",
    "for i in range(SHARD_NUM):\n",
    "    keras.backend.clear_session()\n",
    "    feat_input = Input(shape=(108,), dtype='float32') \n",
    "    hidden=Dense(256,activation='relu')(feat_input)\n",
    "    output=Dense(1,activation='sigmoid')(hidden)\n",
    "    \n",
    "    model = Model([feat_input], output)\n",
    "    \n",
    "    model.compile(loss=['binary_crossentropy'], optimizer=Adam(lr=0.01), metrics=['acc'])\n",
    "    #for m in range(5):\n",
    "    model.fit(alldata[len(traindata)//SHARD_NUM*i:len(traindata)//SHARD_NUM*(i+1)], alllabel[len(traindata)//SHARD_NUM*i:len(traindata)//SHARD_NUM*(i+1)],epochs=5,batch_size=64)\n",
    "    pred=model.predict(alldata[len(traindata):]) \n",
    "    all_pred.append(pred)\n",
    "    print(accuracy_score(alllabel[len(traindata):],np.round(pred)))\n",
    "    all_acc.append(accuracy_score(alllabel[len(traindata):],np.round(pred)))\n",
    "    all_para.append(model.get_weights()) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "old_models=list(all_para)    \n",
    "delete_shard_index= np.random.randint(0,SHARD_NUM-1)\n",
    "delete_sample_index = np.random.randint(0,len(traindata)//SHARD_NUM-1)\n",
    "updated_index=np.array([x for x in np.arange(len(traindata)//SHARD_NUM*delete_shard_index,len(traindata)//SHARD_NUM*(delete_shard_index+1)) if x!=(len(traindata)//SHARD_NUM*delete_shard_index+delete_sample_index)])\n",
    "\n",
    "keras.backend.clear_session()\n",
    "feat_input = Input(shape=(108,), dtype='float32') \n",
    "hidden=Dense(256,activation='relu')(feat_input)\n",
    "output=Dense(1,activation='sigmoid')(hidden)\n",
    "\n",
    "model = Model([feat_input], output) \n",
    "model.compile(loss=['binary_crossentropy'], optimizer=Adam(lr=0.01), metrics=['acc'])\n",
    "model.fit(alldata[updated_index], alllabel[updated_index],epochs=5,batch_size=64)\n",
    "all_para[delete_shard_index]=model.get_weights()\n",
    "    \n",
    "new_para_list=[np.array([x[i] for x in all_para]) for i in range(len(all_para[0]))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "feat_input = Input(shape=(108,), dtype='float32') \n",
    "feat=DNN()(feat_input)\n",
    "model_assemble = Model([feat_input], feat)\n",
    "model_assemble.compile(loss=['binary_crossentropy'], optimizer=Adam(lr=0.05), metrics=['acc'])\n",
    "indexer=np.array(random.sample(list(range(0,len(traindata)//SHARD_NUM*delete_shard_index+delete_sample_index))+ list(range(len(traindata)//SHARD_NUM*delete_shard_index+delete_sample_index+1,len(traindata))),int(len(traindata)//(SHARD_NUM/SAMPLE_DATA_RATIO))))\n",
    "model_assemble.fit(alldata[indexer], alllabel[indexer],epochs=20,batch_size=64)\n",
    "pred=model_assemble.predict(alldata[len(traindata):]) \n",
    "print(accuracy_score(alllabel[len(traindata):],np.round(pred)))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
